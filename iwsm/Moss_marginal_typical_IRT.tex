%***********************************************************************
% IWSM 2026 paper: Marginal vs Typical Task Success
% in Bayesian IRT Analysis of AI Benchmarks
%***********************************************************************

\documentclass[twoside]{report}
\usepackage{iwsm}
\usepackage{graphicx}
\usepackage{amsmath, amssymb}
\usepackage{booktabs}

\begin{document}

\title{Marginal versus typical task success in Bayesian IRT analysis of AI benchmarks}
\titlerunning{Marginal vs typical task success in IRT for AI benchmarks}

\author{Jonas Moss\inst{1}}
\authorrunning{Moss}

\institute{BI Norwegian Business School, Oslo, Norway}

\email{jonas.moss@bi.no}

\abstract{We reanalyze the METR AI agent benchmark using a Bayesian 2PL item response theory model with task difficulty modeled as a random effect. ``Marginal'' success, averaged over the difficulty distribution, is roughly an order of magnitude lower than ``typical'' success at mean difficulty. This gap matters for AI forecasting but is invisible to methods that ignore task-level variation.}

\keywords{Item response theory; AI benchmarks; Bayesian inference; AI safety.}

\maketitle

\section{Introduction}

How quickly are AI systems getting better at real-world tasks? This question has become urgent for governments and AI labs alike. The most influential benchmark for autonomous AI agents is maintained by METR, a non-profit safety organization. Their time horizon analysis drives how labs and regulators alike think about the pace of AI progress. Getting the methodology right matters.

The METR benchmark tests AI agents on about 180 software-engineering tasks ranging from simple file lookups completed in seconds to complex multi-step projects taking hours. Each task has an estimated human completion time $t_j$. Fifteen AI models released between 2023 and 2025 attempt each task, and the outcome is success or failure. The data contains 3343 model-task observations after aggregating repeated runs into counts $y_{ij}$ out of $n_{ij}$ attempts.

The original METR analysis of Kwa et al.\ (2025) proceeds in two stages. First, a logistic regression estimates a time horizon $h_i$ for each AI model, the task length where the model achieves 50\% success. Second, OLS regression of $\log h_i$ on release date yields a doubling time. This is effective but has limitations. The two stages are not jointly estimated, and all tasks of the same human completion time are treated as equally difficult.

We fit a Bayesian 2PL item response theory model (Birnbaum, 1968). Task difficulty is a random effect centered on log human time, so tasks of identical length can differ in difficulty. Model ability follows a parametric trend over release date. We compare four trajectory shapes. The data cannot distinguish them, but the model reveals a large gap between two definitions of success at the 80\% threshold.

\section{Model}

We use a two-parameter logistic IRT model. Each observation is modeled as
\begin{align}
y_{ij} &\sim \text{Bin}(n_{ij},\, p_{ij}), \label{moss:lik} \\
p_{ij} &= \text{logit}^{-1}\!\big(a_j\,(\theta_i - b_j)\big), \label{moss:2pl}
\end{align}
where $\theta_i$ is the ability of model $i$, $b_j$ is the difficulty of task $j$, and $a_j > 0$ is the discrimination of task $j$. Task difficulty depends on human completion time $t_j$ plus a random component. Discrimination has a hierarchical prior. Model ability follows a trend over release date $x_i$ in years, centered at the mean.
\begin{align}
b_j &\sim \mathcal{N}(\alpha + \kappa \log t_j,\; \sigma_b), \label{moss:diff}\\
\log a_j &\sim \mathcal{N}(\mu_a,\; \sigma_a), \label{moss:disc} \\
\theta_i &\sim \mathcal{N}\!\big(f(x_i;\, \gamma),\; \sigma_\theta\big). \label{moss:trend}
\end{align}
We compare four shapes for $f$. Linear, quadratic with $\gamma_2 \geq 0$, power-law, and logistic saturation. Linear $\theta$ growth implies exponential horizon growth due to the logistic link. All models are fitted in Stan (Carpenter et al., 2017) with 4 chains of 1000 post-warmup draws.

\section{Results}

The four trajectory shapes are effectively indistinguishable. ELPD-LOO estimates from Vehtari et al.\ (2017) span only 7 points, from $-2191.9$ for linear to $-2198.7$ for quadratic, all with standard errors around 70. Figure~\ref{moss:fig1} shows the 50\%-success horizon under all four trajectories. They agree over the observed period but diverge sharply in extrapolation. The linear model implies a doubling time of about 7 months, meaning AI capability roughly doubles twice a year.

\begin{figure}[!ht]\centering
\includegraphics[width=\textwidth]{Moss_horizon_fan.pdf}
\caption{\label{moss:fig1} The 50\%-success horizon versus release date under four trajectory models, with 95\% credible bands. All four agree over the observed period but diverge in extrapolation.}
\end{figure}

The estimated difficulty variance $\hat\sigma_b \approx 1.44$ is large. With $\hat\kappa \approx 0.93$, one standard deviation of difficulty corresponds to an $\exp(\sigma_b / \kappa) \approx 4.7$-fold multiplier in equivalent task time.

\section{Marginal versus typical success}
\label{moss:marginal}

There are two ways to define a success horizon. The \emph{typical} horizon asks at what task length a model achieves $p$\% success on a task of average difficulty for that length. This is roughly what METR computes. The \emph{marginal} horizon averages over the distribution of task difficulties at each length. At $p = 50\%$ the two coincide by symmetry of the logistic function. At $p = 80\%$ they diverge substantially. The marginal probability integrates over difficulty,
$$
\bar{p}(\theta, t) = \int \text{logit}^{-1}\!\big(a(\theta - b)\big)\,
\varphi(b \mid \alpha + \kappa \log t,\, \sigma_b)\, db,
$$
and is always pulled toward 50\% relative to the typical value, because the logistic function is concave above its midpoint. The attenuation scales roughly as $\sqrt{1 + \tfrac{\pi^2}{3}(a\sigma_b)^2}$, giving about a factor of 3 in the logit scale. On the time axis this translates to approximately an order of magnitude.

Figure~\ref{moss:fig2} shows this gap for the linear and quadratic trajectories. Both panels show the same qualitative pattern. METR's 80\% horizons implicitly condition on average difficulty, overstating capability. The gap is a level-shift rather than a slope change, so it affects capability estimates more than timeline forecasts.

\begin{figure}[!ht]\centering
\includegraphics[width=\textwidth]{Moss_marginal_typical.pdf}
\caption{\label{moss:fig2} Horizon at 80\% success. Blue dashed is 80\% typical, red solid is 80\% marginal. The shaded gap is roughly an order of magnitude.}
\end{figure}

\section{Discussion}

The marginal-versus-typical distinction matters for AI safety policy. Statements like ``AI can solve $p$\% of $t$-hour tasks'' are ambiguous unless one specifies whether the task has average difficulty or is a random draw. Our analysis treats human times as known rather than latent, likely understating uncertainty. The explanatory IRT framework of De Boeck and Wilson (2004) could accommodate additional task covariates.

\references
\begin{description}
\item[Birnbaum, A.] (1968).
     Some latent trait models and their use in inferring an examinee's
     ability. In: {\it Statistical Theories of Mental Test Scores},
     Addison-Wesley, 395\,--\,479.
\item[Carpenter, B.\ et al.] (2017).
     Stan: a probabilistic programming language.
     {\it J.\ Stat.\ Softw.}, {\bf 76}(1), 1\,--\,32.
\item[De Boeck, P.\ and Wilson, M.] (2004).
     {\it Explanatory Item Response Models}.
     New York: Springer.
\item[Kwa, T., Bloomfield, A., Dao, L., Meinke, A.,]
     and Rein, D.\ (2025). Measuring AI ability to complete
     long tasks. arXiv:2503.14499.
\item[Vehtari, A., Gelman, A., and Gabry, J.] (2017).
     Practical Bayesian model evaluation using leave-one-out
     cross-validation and WAIC.
     {\it Stat.\ Comput.}, {\bf 27}(5), 1413\,--\,1432.
\end{description}

\end{document}
