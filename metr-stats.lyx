#LyX 2.4 created this file. For more info see https://www.lyx.org/
\lyxformat 620
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\use_default_options true
\maintain_unincluded_children no
\language american
\language_package default
\inputencoding utf8
\fontencoding auto
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_roman_osf false
\font_sans_osf false
\font_typewriter_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\float_placement class
\float_alignment class
\paperfontsize default
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_formatted_ref 0
\use_minted 0
\use_lineno 0
\index Index
\shortcut idx
\color #008080
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tablestyle default
\tracking_changes false
\output_changes false
\change_bars false
\postpone_fragile_content true
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\docbook_table_output 0
\docbook_mathml_prefix 1
\end_header

\begin_body

\begin_layout Title
METR data modelling
\end_layout

\begin_layout Author
Jonas Moss
\end_layout

\begin_layout Itemize
Goal:
 Modeling of METR trajectory.
\end_layout

\begin_layout Itemize
Method:
 2PL with direct modeling of everything.
 
\end_layout

\begin_layout Itemize
The basic 2PL:
 What and why
\end_layout

\begin_layout Itemize
Adding dates to stuff.
\end_layout

\begin_layout Itemize
Potential bla bla bla?
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
\begin_inset Quotes eld
\end_inset

We want to forecast when AI systems will be able to do tasks of difficulty X,
 here's the principled measurement model for doing that,
 and it happens to also give us an interpretable scale as a bonus.
\begin_inset Quotes erd
\end_inset


\end_layout

\begin_layout Standard
One of the cool feautres of the METR presention is how 
\begin_inset Quotes eld
\end_inset

latent ability
\begin_inset Quotes erd
\end_inset

 is transformed into a natural scale.
 Say IQ measures intelligence,
 but is arbitrarily scaled to have a mean of 
\begin_inset Formula $100$
\end_inset

 and a standard deviation of 
\begin_inset Formula $15$
\end_inset

,
 and its not straight-forward to say something like 
\begin_inset Quotes eld
\end_inset

intelligence responds linearly to compute input
\begin_inset Quotes erd
\end_inset

,
 as,
 well,
 that depends on how we actually measure it.
 (This,
 of course,
 also holds for stuff like Sam Altman's claim that LLM capabilities respond logarithmically to compute.
 Yes,
 you can probably claim that about a specific measurement such as perplexity,
 but generic capabilities is another story.) Epoch also has this problem,
 where GPT-4 is set to 
\begin_inset Formula $100$
\end_inset

 and Sonnet 3.7 is set to 
\begin_inset Formula $130$
\end_inset

.
 What does an increase of 
\begin_inset Formula $30$
\end_inset

 more points mean?
 The same as the capability difference between GPT-4 and Sonnet 3.7?
 Not really.
 
\end_layout

\begin_layout Standard
This leads us into the natural measurement model for METR data,
 the 2-parameter logistic model.
 Let 
\begin_inset Formula $i$
\end_inset

 index the models,
 
\begin_inset Formula $j$
\end_inset

 index tasks,
 and 
\begin_inset Formula $k$
\end_inset

 index a potential repeat of a task.
 If 
\begin_inset Formula $Y_{ijk}=1$
\end_inset

 when model 
\begin_inset Formula $i$
\end_inset

 succeeds on the 
\begin_inset Formula $k$
\end_inset

th repeat of task 
\begin_inset Formula $j$
\end_inset

,
 then we model 
\begin_inset Formula 
\[
P(Y_{ijk}=1)=\sigma[\beta_{j}(\theta_{i}-\delta_{j})],
\]

\end_inset

where 
\begin_inset Formula $\sigma=1/(1+e^{-x})$
\end_inset

 is the logistic,
 
\begin_inset Formula $\delta_{j}$
\end_inset

 is the item difficulty,
 
\begin_inset Formula $\theta_{i}$
\end_inset

 is model ability,
 and 
\begin_inset Formula $\beta_{j}>0$
\end_inset

 is the task discrimination.
 
\end_layout

\begin_layout Standard
The problem right now is that 
\begin_inset Formula $\theta_{i}$
\end_inset

 isn't directly interoperable,
 just as before.
 The phenomenal thing about METR plots is associating 
\begin_inset Formula $\delta_{j}$
\end_inset

 with the 
\emph on
human time
\emph default
 of the 
\begin_inset Formula $j$
\end_inset

th task,
 which allows us to anchor the difficulties,
 and by consequence,
 the model abilities 
\begin_inset Formula $\theta_{i}$
\end_inset

.
 The second step is associating the model abilities with their release dates 
\begin_inset Formula $d_{i}$
\end_inset

.
\end_layout

\begin_layout Standard
Concretely,
 let's do the following modeling steps:
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & = & a_{\theta}+b_{\theta}\log d_{i}+u_{i},\\
\delta_{j} & = & a_{\delta}+b_{\delta}\log t_{j}+v_{j}.
\end{eqnarray*}

\end_inset

Here 
\begin_inset Formula $u_{i},v_{i}$
\end_inset

 are residuals that can account for some off-slope behavior.
 
\end_layout

\begin_layout Standard
(This is one possible model,
 and 
\emph on
both
\emph default
 models matter.
 The model for 
\begin_inset Formula $\delta_{j}$
\end_inset

 tells us the correct scaling of the difficulties,
 the model for 
\begin_inset Formula $\theta_{i}$
\end_inset

 the correct scaling for the dates,
 and both should be verified or checked in one way or another.)
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Short overview:
 METR trend extrapolation is important.
 The way METR does it in their paper good but somewhat ad hoc.
 Here's a principled alternative that doubles down on item response theory.
 
\end_layout

\begin_layout Standard
Lets have a look at an idealized variant of the METR data,
 which looks roughly like this:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\[
\left[\begin{array}{ccccccc}
\text{Model} & \text{Task ID} & \text{Success} & \text{Attempts} & \text{Date} & \text{Family} & \text{Human time}\\
\text{o3} & 1 & 3 & 4 & 2025-04-16 & a\\
\text{Opus 4} & 2 & 3 & 4 & 2025-05-22 & b\\
\vdots\\
\text{Opus 3} & 3 & 3 & 4 & 2024-03-04 & c\\
\text{o1} & 4 & 3 & 4 & 2024-12-05 & d
\end{array}\right]
\]

\end_inset

So there are a bunch of (model,task) pairs that have have.
\end_layout

\begin_layout Section
2PL model
\end_layout

\begin_layout Standard
Let 
\begin_inset Formula $i,j,k$
\end_inset

 be the index of the model,
 the task,
 and the independent run of the task.
 The binary variables 
\begin_inset Formula $Y_{ijk}$
\end_inset

 indicate task success.
 We need to model the probability of task completion.
 Our 2PL model family looks like
\begin_inset Formula 
\[
P(Y_{ijk}=1)=\sigma[\beta_{j}(\theta_{i}-\delta_{j})],
\]

\end_inset

where 
\begin_inset Formula $\sigma=1/(1+e^{-x})$
\end_inset

 is the logistic,
 
\begin_inset Formula $\delta_{j}$
\end_inset

 is the item difficulty,
 
\begin_inset Formula $\theta_{i}$
\end_inset

 is model ability,
 and 
\begin_inset Formula $\beta_{j}>0$
\end_inset

 is the task discrimination.
 
\end_layout

\begin_layout Standard
This is called a 2-parameter logistic model (2PL) in item response theory.
\end_layout

\begin_layout Itemize
The ability or skill parameters 
\begin_inset Formula $\theta_{i}$
\end_inset

 for model 
\begin_inset Formula $i$
\end_inset

 (e.g.,
 GPT-4o) is small if the model is a poor and large if its good.
\end_layout

\begin_layout Itemize
If the discrimination parameter is large:
 A positive difference 
\begin_inset Formula $\theta_{i}-\delta_{j}$
\end_inset

 will cause the resulting probability to be large as well,
 a negative difference will cause the resulting probability to be small.
 
\end_layout

\begin_deeper
\begin_layout Itemize
Its easiest to think of this from a human perspective.
 Lets say you have a have a high school calculus problem such as 
\begin_inset Quotes eld
\end_inset

what is the derivative of 
\begin_inset Formula $\sin x$
\end_inset


\begin_inset Quotes erd
\end_inset

.
 The ok students will virtually always know the answers to this problem,
 i.e.,
 the students that paid attention at all.
 The other students will almost always fail.
 There is little room for randomness,
 so the discrimination is large.
 
\end_layout

\end_deeper
\begin_layout Standard
From these we get a posterior for the relevant parameters.
 Now we have time horizons 
\begin_inset Formula $t_{j}$
\end_inset

 for each task and release dates 
\begin_inset Formula $d_{i}$
\end_inset

 for the models as well.
 We treat those as deterministic.
 We'd like to estimate
\begin_inset Formula 
\[
p_{i}(t)=P(\text{model }i\text{ succeeds on task of length }t).
\]

\end_inset

Suppose 
\begin_inset Formula $\beta,\theta,\delta$
\end_inset

 are known first of course.
 We need to map 
\begin_inset Formula $t_{j}$
\end_inset

 to 
\begin_inset Formula $(\beta_{j},\delta_{j})$
\end_inset

 then.
 The problem is that we do not know this mapping or if it even exists in a reasonable way 
\begin_inset Formula $\beta_{j},\delta_{j}$
\end_inset

.
 It would be possible to connect these,
 maybe,
 by having more participants and potentially censored time-to-completion distributions.
 This could potentially make data-collecting for METR easier too.
 
\end_layout

\begin_layout Standard
Anyway,
 lets just think of the easiest version:
\begin_inset Formula 
\[
p_{i}(t_{j})=P(Y_{ijk}|\beta_{j},\delta_{j}).
\]

\end_inset

Here we get 
\emph on
something
\emph default
.
 Will it look reasonable,
 e.g.,
 monotonic?
 Probably not,
 due to different 
\begin_inset Formula $\beta_{j}$
\end_inset

 and 
\begin_inset Formula $\delta_{j}$
\end_inset

 behaving erratically.
\end_layout

\begin_layout Section
More modelling
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{eqnarray*}
\theta_{i} & = & f(d_{i})+\epsilon_{i}
\end{eqnarray*}

\end_inset


\end_layout

\begin_layout Section
Joint 2PL model
\end_layout

\begin_layout Standard
Lets use the 2PL model but also include 
\begin_inset Formula 
\begin{eqnarray*}
\delta_{j} & = & f(t_{j})+u_{j},\quad\beta_{j}=g(t_{j})+v_{j},
\end{eqnarray*}

\end_inset

for some reasonable functions 
\begin_inset Formula $f,g$
\end_inset

.
\end_layout

\begin_layout Standard
Two reasonable choices are 
\begin_inset Formula $\delta_{j}=\log t_{j}+u_{j}$
\end_inset

 and 
\begin_inset Formula $\beta_{j}$
\end_inset

 constant.
 In this case we get very close to the METR model actually,
 only difference is we have a random effect in difficulty,
 which does look reasonable.
 
\begin_inset Formula 
\begin{eqnarray*}
P(Y_{ijk}=1) & = & \sigma[\beta_{j}(\theta_{i}-\delta_{j})],\\
\delta_{j} & = & a_{\delta}+b_{\delta}\log t_{j}+u_{j},\\
\beta_{j} & = & a_{\beta}+b_{\beta}\log t_{j}+v_{j}.
\end{eqnarray*}

\end_inset

And then we have 
\begin_inset Formula $S_{\theta}(t)=P(Y=1|t,\theta)=\int\sigma[\beta_{j}(\theta-\delta_{j})]dp(u)dp(v)$
\end_inset

.
 Ok so what do we want here?
 We want to match 
\begin_inset Formula $(d_{i},\theta_{i})$
\end_inset

 to some kind of 
\begin_inset Formula $t$
\end_inset

.
 And what we have is actually 
\begin_inset Formula $S_{i}$
\end_inset

.
 So we want to do model 
\begin_inset Formula $S_{g(d)}(t)$
\end_inset

 somehow,
 and that is easies through 
\begin_inset Formula $\sum_{i=1}^{n}(\theta_{i}-g(d))^{2}$
\end_inset

 no?
\end_layout

\begin_layout Section
Monotone regression model
\end_layout

\end_body
\end_document
