---
title: "Analyzing the METR data"
author: "Jonas Moss"
date: "2026-02-XX"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
    embed-resources: true
execute:
  echo: true
  warning: false
  freeze: auto
jupyter: python3
---

```{python}
# | echo: false
# | output: false
# | label: setup

import json
import math
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime

# ---------------------------------------------------------------------------
# Paths & specs
# ---------------------------------------------------------------------------
RUNS_ROOT = Path("outputs/runs")

SPECS = {
    "Linear": "time_irt__theta_linear",
    "Quadratic": "time_irt__theta_quadratic_pos",
    "Power-law": "time_irt__theta_xpow",
    "Saturating": "time_irt__theta_theta_logistic",
}

SPEC_COLORS = {
    "Linear": "C0",
    "Quadratic": "C1",
    "Power-law": "C2",
    "Saturating": "C3",
}


def resolve_latest(spec: str) -> Path:
    """Return the concrete run directory for a spec (reads LATEST pointer)."""
    spec_dir = RUNS_ROOT / spec
    run_id = (spec_dir / "LATEST").read_text().strip()
    return spec_dir / run_id


def figdir(spec: str) -> Path:
    return resolve_latest(spec) / "figures"


def diagdir(spec: str) -> Path:
    return resolve_latest(spec) / "diagnostics"


# ---------------------------------------------------------------------------
# Utility functions (from scripts/make_figures.py)
# ---------------------------------------------------------------------------
def logistic(x):
    return 1.0 / (1.0 + np.exp(-np.asarray(x, dtype=float)))


def logit(p):
    return math.log(p / (1.0 - p))


def _format_duration_hours(hours: float) -> str:
    if not np.isfinite(hours) or hours <= 0:
        return ""
    seconds = hours * 3600.0
    if seconds < 60:
        return f"{int(round(seconds))} sec"
    minutes = seconds / 60.0
    if minutes < 60:
        return f"{int(round(minutes))} min"
    if hours < 24:
        v = int(round(hours))
        return f"{v} hour" if v == 1 else f"{v} hours"
    days = hours / 24.0
    if days >= 365:
        years = int(round(days / 365.0))
        return f"{years} year" if years == 1 else f"{years} years"
    if days >= 30:
        months = int(round(days / 30.0))
        return f"{months} month" if months == 1 else f"{months} months"
    v = int(round(days))
    return f"{v} day" if v == 1 else f"{v} days"


def apply_metr_duration_ticks(ax, y_values_hours):
    y = np.asarray(y_values_hours, dtype=float)
    y = y[np.isfinite(y) & (y > 0)]
    if y.size == 0:
        return
    ymin, ymax = float(np.min(y)), float(np.max(y))
    if ymin <= 0 or ymax <= 0:
        return
    ticks = np.array(
        [
            4 / 3600,
            36 / 3600,
            6 / 60,
            1.0,
            10.0,
            4 * 24.0,
            30 * 24.0,
            6 * 30 * 24.0,
            3 * 365 * 24.0,
            18 * 365 * 24.0,
            50 * 365 * 24.0,
            100 * 365 * 24.0,
            500 * 365 * 24.0,
        ]
    )
    lo, hi = ymin / 1.15, ymax * 1.15
    keep = ticks[(ticks >= lo) & (ticks <= hi)]
    if keep.size < 3:
        below = ticks[ticks < lo]
        above = ticks[ticks > hi]
        candidates = []
        if below.size:
            candidates.append(below[-1])
        candidates.extend(keep.tolist())
        if above.size:
            candidates.append(above[0])
        keep = np.array(candidates, dtype=float)
    if keep.size == 0:
        return
    ax.set_yticks(keep)
    ax.set_yticklabels([_format_duration_hours(float(t)) for t in keep])
    ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)


# ---------------------------------------------------------------------------
# Data loaders
# ---------------------------------------------------------------------------
def load_theta_points(spec: str) -> pd.DataFrame:
    df = pd.read_csv(figdir(spec) / "theta_points.csv")
    df["release_date"] = pd.to_datetime(df["release_date"])
    return df


def load_theta_trend(spec: str) -> pd.DataFrame:
    df = pd.read_csv(figdir(spec) / "theta_trend_grid.csv")
    df["date"] = pd.to_datetime(df["date"])
    return df


def load_horizon_grid(spec: str, kind: str = "marginal") -> pd.DataFrame:
    fname = "horizon_grid.csv" if kind == "marginal" else "horizon_grid_typical.csv"
    df = pd.read_csv(figdir(spec) / fname)
    df["date"] = pd.to_datetime(df["date"])
    return df


def load_horizon_points(spec: str, kind: str = "marginal") -> pd.DataFrame:
    fname = "horizon_points.csv" if kind == "marginal" else "horizon_points_typical.csv"
    df = pd.read_csv(figdir(spec) / fname)
    df["release_date"] = pd.to_datetime(df["release_date"])
    return df


def load_meta(spec: str) -> dict:
    return json.loads((resolve_latest(spec) / "fit" / "meta.json").read_text())


# ---------------------------------------------------------------------------
# Plot defaults
# ---------------------------------------------------------------------------
plt.rcParams.update(
    {
        "figure.facecolor": "white",
        "axes.facecolor": "white",
        "font.size": 11,
        "axes.titlesize": 13,
        "axes.labelsize": 12,
    }
)
```

Let's start with a plot that shouldn't be too surprising. Four models fit the METR data equally well. They agree about the past but disagree about the future.

```{python}
# | label: fig-horizon-fan
# | fig-cap: "50%-success horizon vs. release date under four trajectory models that are statistically indistinguishable on current data.
# | echo: false

fig, ax = plt.subplots(figsize=(9.5, 3.8))

x_max = pd.Timestamp("2029-01-01")

for label, spec_name in SPECS.items():
    g = load_horizon_grid(spec_name).query("p == 0.5").sort_values("date")
    g = g[g["date"] <= x_max]
    ax.plot(
        g["date"], g["t_hours_q50"], color=SPEC_COLORS[label], linewidth=2, label=label
    )
    ax.fill_between(
        g["date"],
        g["t_hours_q10"],
        g["t_hours_q90"],
        color=SPEC_COLORS[label],
        alpha=0.10,
    )

# Model-specific horizon points.
hpts = load_horizon_points(list(SPECS.values())[0]).query("p == 0.5")
ax.scatter(
    hpts["release_date"], hpts["t_hours_q50"], s=30, color="black", alpha=0.7, zorder=5
)
for _, r in hpts.iterrows():
    ax.plot(
        [r["release_date"], r["release_date"]],
        [r["t_hours_q10"], r["t_hours_q90"]],
        color="black",
        alpha=0.3,
        linewidth=1.5,
    )

# Label a few key models.
LABELS_TO_SHOW = {
    "gpt_4o_inspect": ("GPT-4o", (8, -8)),
    "claude_3_5_sonnet_20241022_inspect": ("Sonnet 3.5 (new)", (8, -8)),
    "claude_opus_4_5_inspect": ("Opus 4.5", (8, 0)),
}
for _, r in hpts.iterrows():
    if r["model"] in LABELS_TO_SHOW:
        txt, ofs = LABELS_TO_SHOW[r["model"]]
        ax.annotate(
            txt,
            (r["release_date"], r["t_hours_q50"]),
            textcoords="offset points",
            xytext=ofs,
            fontsize=7.5,
            color="0.3",
            va="center",
        )

# 1-month reference line.
ax.axhline(
    30 * 24, color="red", linestyle="--", alpha=0.5, linewidth=1.5, label="1 month"
)

ax.set_yscale("log")
ax.set_xlabel("Release date")
ax.set_title("50%-success horizon vs. release date")
ax.set_xlim(pd.Timestamp("2023-01-01"), x_max)
ax.set_ylim(0.03, 365 * 24)  # ~2 min to 1 year
ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))

# Duration ticks within the visible range.
tick_hours = np.array(
    [
        6 / 60,
        1.0,
        10.0,
        4 * 24.0,
        30 * 24.0,
        6 * 30 * 24.0,
    ]
)
ax.set_yticks(tick_hours)
ax.set_yticklabels([_format_duration_hours(float(t)) for t in tick_hours])
ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)

ax.legend(fontsize=8, loc="upper left")
fig.tight_layout()
plt.show()
```

The linear, quadratic, and power-law trajectories all predict AI will cross the one-month horizon within the next few years, but the saturating model --- which fits equally well --- says it may never happen. The honest answer is that we don't know which curve is right, and the data so far can't tell us. Let me back up and explain where these curves come from.

METR's analysis of the time-horizon data proceeds in two stages:

1. **Per-model logistic regression.** For each model $i$, fit $P(\text{success}) = \sigma(\beta_i(\log h_i - \log t_j))$ where $t_j$ is human time for task $j$. Here $h_i$ is the task duration where the curve crosses 50% --- when $t_j = h_i$ the argument is zero, so $\sigma(0) = 0.5$. This gives a "horizon score" $h_i$ per model.

2. **An OLS trend.** Regress $\log h_i$ on release date. The slope gives a doubling time of $\approx 4$ months.

This is clean and gets the main story right, but there are some non-standard choices. For instance, the slope $\beta_i$ is per-model rather than per-task (which is unusual), Stage-2 uncertainty doesn't propagate back into Stage 1, and there is only one trajectory shape. None of these are fatal, and the two-stage approach is a perfectly reasonable starting point.

In this post I make a joint model (one model instead of two stages), adjust some things to be more in line with standard practice, and ask what happens when you try different trajectory shapes. The post is unavoidably somewhat technical, but not so godawful that Claude won't be able to answer any question you have about the methodology. Models are fitted with Stan and code is available [here]. I won't go into details about prior choices; the code has them all. (All priors were chosen by Codex / Claude Code and appear reasonable enough.)

## Psychometrics

METR's model is *almost* a 2-parameter logistic model (2PL), the bread-and-butter of educational testing since the 1960s.

So, what kind of problems were the 2PL model designed for? Say you give 200 students a maths exam with
50 questions and record their answers as correct / incorrect. Some
students are strong, some are weak, and some questions are easy, some are hard. You want to
estimate the students' math ability, but raw percent correct scores aren't necessarily very good, as they depend on which questions happened to be on the exam. 

The 2PL model solves this by giving each student a single ability score ($\theta_i$) and each
question two parameters: a *difficulty* ($b_j$, how hard it is) and a *discrimination*
($a_j$, how cleanly it separates strong from weak students). "What is 2+2?" has low
discrimination --- everyone gets it right regardless of ability. A tricky proof-writing question has high
discrimination --- strong students nail it, weak students have no chance.
The model estimates all of these simultaneously via a logistic regression:

$$
P(\text{success} \mid \text{model } i, \text{task } j) = \text{logit}^{-1}\bigl(a_j (\theta_i - b_j)\bigr)
$$

The reason this machinery matters here is that METR tasks are like exam questions. They vary in both difficulty and discriminating power, and we need to place AI models on a common ability scale.

## Modeling difficulty

A standard problem with 2PL is that the ability and difficulty parameters $\theta_i, b_j$ are hard to interpret --- same problem as IQ. The scale is arbitrary and it's not clear what a 0.1 increase in ability actually means. A cool feature of the METR data is that each task comes with a human time, which is already scaled and interpretable. Longer tasks tend to be harder, but not always --- a 10-hour coding task might be trivial while a 10-hour research task could be impossible. So I model difficulty as:

$$
b_j \sim \mathcal{N}(\alpha + \kappa \cdot \log t_j, \;\sigma_b)
$$

Each task's difficulty has a mean that depends on log-length, plus a random component to account for the fact that same-length tasks are not born equal. (METR treats all tasks of identical length as equally hard.) We'll see that $\sigma_b \approx 1.44$, which is large --- two tasks with the same human-time can differ by several units on the difficulty scale. This matters mostly for uncertainty: compared to the simpler $b_j = \alpha + \kappa \cdot \log t_j$, the random effects widen the credible intervals.

This is a modelling choice that can be wrong. There's no guarantee that difficulty is well-approximated by a linear function of $\log t_j$, so we need diagnostics to check. Here's a residual plot which I find reasonable, with some minor caveats.

```{python}
#| label: fig-difficulty-residuals
#| fig-cap: "Each dot is a task. Residual difficulty (y-axis) measures how much harder or easier a task is for AI than predicted by human-time alone. The spread is large (σ_b ≈ 1.44) but there is no systematic curvature."
#| echo: false

resid = pd.read_csv(
    diagdir("time_irt__theta_linear") / "difficulty_residuals.csv"
)

fig, ax = plt.subplots(figsize=(7, 3.5))
ax.scatter(resid["x"], resid["u_hat"], s=18, alpha=0.45, color="C0", edgecolors="none")
ax.axhline(0, color="gray", linestyle="--", linewidth=0.8, alpha=0.6)
ax.set_xlabel(r"Centred log task length  $x_j = \log(t_j) - \overline{\log t}$")
ax.set_ylabel(r"Residual difficulty  $\hat{u}_j$")
fig.tight_layout()
plt.show()
```

Diagnostics aside, this is what $\sigma_b \approx 1.44$ looks like in practice. Human-time tells you something real about AI-difficulty, but there's a lot of spread. No obvious curvature, so the log-linear form is fine as a first approximation. There's a weird cluster at the left --- very easy tasks that contain virtually no information about difficulty. I don't think it's worth inspecting these further. Overall this looks a-ok to me.

Since difficulty is a function of log-task-length, we can convert each model's ability into a 50%-success horizon. Here's every model in the dataset, with no trajectory imposed. Notice that the credible intervals are pretty tight on this scale. The hard part isn't estimation of model capabilities, it's extrapolation. 

```{python}
#| label: fig-horizon-baseline
#| fig-cap: "Per-model 50%-success horizon. Each point is a model; bars show 10-90% credible intervals."
#| echo: false

BASELINE_DIR = Path("outputs/runs/20260206_120053/figures")
bpts = pd.read_csv(BASELINE_DIR / "horizon_points.csv")
bpts["release_date"] = pd.to_datetime(bpts["release_date"])
bpts = bpts.query("p == 0.5").sort_values("release_date")

MODEL_LABELS = {
    "gpt_4": "GPT-4",
    "gpt_4_1106_inspect": "GPT-4 Turbo (1106)",
    "gpt_4_turbo_inspect": "GPT-4 Turbo",
    "gpt_4o_inspect": "GPT-4o",
    "claude_3_opus_inspect": "Claude 3 Opus",
    "claude_3_5_sonnet_20240620_inspect": "Sonnet 3.5",
    "claude_3_5_sonnet_20241022_inspect": "Sonnet 3.5 (new)",
    "o1_preview": "o1-preview",
    "o1_inspect": "o1",
    "claude_3_7_sonnet_inspect": "Claude 3.7",
    "o3_inspect": "o3",
    "claude_4_opus_inspect": "Opus 4",
    "gpt_5_2025_08_07_inspect": "GPT-5",
    "claude_opus_4_5_inspect": "Opus 4.5",
}

fig, ax = plt.subplots(figsize=(9.5, 4.0))

ax.errorbar(
    bpts["release_date"], bpts["t_hours_q50"],
    yerr=np.vstack([
        bpts["t_hours_q50"] - bpts["t_hours_q10"],
        bpts["t_hours_q90"] - bpts["t_hours_q50"],
    ]),
    fmt="o", color="C0", ecolor="0.35",
    elinewidth=1.5, capsize=3, alpha=0.85, zorder=5,
    markersize=6,
)

# Label each point.
for _, r in bpts.iterrows():
    lbl = MODEL_LABELS.get(r["model"], r["model"])
    ax.annotate(
        lbl,
        (r["release_date"], r["t_hours_q50"]),
        textcoords="offset points",
        xytext=(6, 4),
        fontsize=6.5,
        color="0.35",
        va="bottom",
    )

ax.set_yscale("log")
ax.set_xlabel("Release date")
ax.set_ylabel("50%-success horizon")

# Duration ticks (mimic METR style).
tick_hours = np.array([4/3600, 36/3600, 6/60, 1.0, 10.0])
ax.set_yticks(tick_hours)
ax.set_yticklabels([_format_duration_hours(float(t)) for t in tick_hours])
ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)

ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))
fig.tight_layout()
plt.show()
```


## Modeling model ability

By directly modeling model ability we can try several functional shapes: exponential, subexponential, superexponential, saturating, and singularity. Forecasts depend a lot on which shape you pick, and the data doesn't really tell you much, so it's not easy to choose between them. Your priors rule here.

Define $x_i = (\text{date}_i - \text{date}_0) / 365.25$, where $\text{date}_0$ is the mean release date (currently Sep 2024), so $x$ is in years. Note that $x$ is *not* logged --- since difficulty already scales with log task-length, a linear $\theta(x)$ already implies exponential growth in horizon. The abilities are modeled as
$$
\theta_i \sim \mathcal{N}\bigl(f(x_i;\, \gamma),\; \sigma_\theta\bigr)
$$

I'm still using a random effect for model ability here, since nobody seriously thinks every model released on the same date must be equally capable. I'm looking at four shapes for $f$:

| Model | $f(x)$ | Params | Intuition |
|:------|:-------|:------:|:----------|
| Linear | $\gamma_0 + \gamma_1 x$ | 2 | Linear $\theta$ = exponential horizon growth (constant doubling time) |
| Quadratic | $\gamma_0 + \gamma_1 x + \gamma_2 x^2$, $\gamma_2 \geq 0$ | 3 | Superexponential, accelerating growth |
| Power-law | $\gamma_0 + \gamma_1 \tilde{x}^{\alpha}$, $\alpha \in [0.1, 2]$ | 3 | Flexible: sub- or super-exponential |
| Saturating | $\theta_{\min} + \Delta\theta \cdot \text{logit}^{-1}(a + bx)$ | 4 | S-curve ceiling on ability |

Here $\tilde{x}$ is a shifted/scaled version of $x$ to ensure positivity (you can't raise a negative number to a fractional power). The saturating model is a plain sigmoid in θ-space --- ability itself follows an S-curve that plateaus at $\theta_{\min} + \Delta\theta$.

If METR's GitHub repo contained all the historical data I would also have tried a piecewise linear with a breakpoint around the time of o1, which visually fits the original METR graphs better than a plain linear fit. Since the available data doesn't go that far back I don't need to. Getting hold of the latest data points is more important.

(Quadratic and power law are simple choices for super- and subexponential growth, but I suspect the story would be similar for any other reasonably flexible function --- the data looks roughly linear and we just need to fit that well enough.)

All models share the same 2PL likelihood and task parameters ($b_j$, $a_j$,
$\alpha$, $\kappa$, $\sigma_b$). Only the $\theta$-prior changes. Each is
fitted with 4 chains $\times$ 1000 post-warmup draws in Stan.

## Results

I compared all four models using leave-one-out cross-validation (LOO-CV). The punchline: they all fit the data about equally well. The LOO scores differ by at most ~7 points --- well within noise --- and calibration is nearly identical (Brier $\approx$ 0.066 across the board). The saturating model, despite producing radically different forecasts, is indistinguishable from the linear model on in-sample fit. This is the core problem: the data we have simply cannot tell us which trajectory shape is correct.


### Implied doubling time

Under the linear model, $\theta$ grows at a constant rate, which means exponential growth in horizon. The **doubling time** --- how long until the 50% horizon doubles --- falls out of the slope $\gamma_1$.

I get **4.1 months** (80% CrI: 3.7--4.6), consistent with METR's v1.1 estimate of ~4.3 months. The posterior is tight because the IRT model pools across tasks --- but this is *conditional on the linear trajectory being correct*.

```{python}
#| label: fig-doubling-time
#| fig-cap: "Posterior distribution of the horizon doubling time under the linear model. Median: 4.1 months (80% CrI: 3.7–4.6 months)."
#| echo: false

from scipy.stats import gaussian_kde

draws = pd.read_csv(
    figdir("time_irt__theta_linear") / "doubling_time_draws.csv"
)
dt_months = draws["doubling_time_years"].values * 12

kde = gaussian_kde(dt_months, bw_method="silverman")
x = np.linspace(dt_months.min() - 0.3, dt_months.max() + 0.3, 400)

q10, q50, q90 = np.percentile(dt_months, [10, 50, 90])

fig, ax = plt.subplots(figsize=(7, 3.2))
ax.fill_between(x, kde(x), alpha=0.15, color="C0")
ax.plot(x, kde(x), color="C0", linewidth=2)
ax.axvline(q50, color="black", linewidth=1.5, label=f"Median = {q50:.1f} months")
ax.axvline(q10, color="black", linewidth=1, linestyle="--", alpha=0.6,
           label=f"80% CrI: [{q10:.1f}, {q90:.1f}]")
ax.axvline(q90, color="black", linewidth=1, linestyle="--", alpha=0.6)
ax.set_xlabel("Doubling time (months)")
ax.set_ylabel("Posterior density")
ax.legend(fontsize=9)
fig.tight_layout()
plt.show()
```


### What about 80% success? Typical vs. marginal

Everything above uses 50% success, but METR also cares about 80% success and fits a separate model for that. We don't need to do that here since the model estimation doesn't really depend on that sort of thing. We'll just calculate the 80% success instead.

But there's two ways to define "80% success," and they give different answers.

1. **Typical:** Pick a task of average difficulty for its length. Can the model solve it 80% of the time? This is roughly what METR computes.

2. **Marginal:** Pick a random task of that length. What's the expected success rate? Because some tasks are much harder than average ($\sigma_b \approx 1.44$ is large!), the hard ones drag down the average more than easy ones push it up.

At 50% the two definitions agree. But at 80%, the gap is roughly an order of magnitude:

```{python}
# | label: fig-marginal-typical
# | fig-cap: "Horizon forecasts at 80% success probability, comparing the 'typical' definition (average-difficulty task) with the 'marginal' definition (random task). The gap reflects task-difficulty variance (σ_b ≈ 1.44). Left: linear trajectory. Right: quadratic (constrained)."
# | echo: false

panel_specs = [
    ("Linear", "time_irt__theta_linear"),
    ("Quadratic", "time_irt__theta_quadratic_pos"),
]
x_max = pd.Timestamp("2029-01-01")

fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.0), sharey=True)

for ax, (panel_label, spec_name) in zip(axes, panel_specs):
    marg = load_horizon_grid(spec_name, kind="marginal")
    typ = load_horizon_grid(spec_name, kind="typical")

    # 50% (marginal ≈ typical) — thin reference.
    g50 = marg.query("p == 0.5").sort_values("date")
    g50 = g50[g50["date"] <= x_max]
    ax.plot(
        g50["date"], g50["t_hours_q50"], color="0.6", linewidth=1.2, label="50% success"
    )
    ax.fill_between(
        g50["date"], g50["t_hours_q10"], g50["t_hours_q90"], color="0.6", alpha=0.06
    )

    # 80% typical.
    g80t = typ.query("p == 0.8").sort_values("date")
    g80t = g80t[g80t["date"] <= x_max]
    ax.plot(
        g80t["date"], g80t["t_hours_q50"], color="C0", linewidth=2, label="80% typical"
    )
    ax.fill_between(
        g80t["date"], g80t["t_hours_q10"], g80t["t_hours_q90"], color="C0", alpha=0.10
    )

    # 80% marginal.
    g80m = marg.query("p == 0.8").sort_values("date")
    g80m = g80m[g80m["date"] <= x_max]
    ax.plot(
        g80m["date"],
        g80m["t_hours_q50"],
        color="C3",
        linewidth=2,
        linestyle="--",
        label="80% marginal",
    )
    ax.fill_between(
        g80m["date"], g80m["t_hours_q10"], g80m["t_hours_q90"], color="C3", alpha=0.10
    )

    ax.axhline(
        30 * 24, color="red", linestyle="--", alpha=0.3, linewidth=1, label="1 month"
    )

    ax.set_yscale("log")
    ax.set_title(panel_label, fontsize=12)
    ax.set_xlim(pd.Timestamp("2023-01-01"), x_max)
    ax.set_ylim(1e-3, 365 * 24)
    ax.xaxis.set_major_locator(mdates.YearLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))

    tick_hours = np.array(
        [1 / 60, 6 / 60, 1.0, 10.0, 4 * 24.0, 30 * 24.0, 6 * 30 * 24.0]
    )
    ax.set_yticks(tick_hours)
    ax.set_yticklabels([_format_duration_hours(float(t)) for t in tick_hours])
    ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)

axes[0].set_ylabel("")
axes[0].legend(fontsize=7.5, loc="upper left")
axes[1].set_xlabel("Release date")
axes[0].set_xlabel("Release date")
fig.tight_layout()
plt.show()
```

This gap is entirely driven by task-difficulty variance ($\sigma_b \approx 1.44$). The difference is not really a consequence of modelling, but in the data itself, as tasks of the same human-time length vary enormously in how hard they are for AI.

I'd say the "correct" horizon to talk about is the marginal one. Typical is optimistic since it assumes an average-difficulty task. Marginal is conservative and accounts for the full spread of real tasks.


### When does the horizon cross key thresholds?

For each trajectory, I compute the posterior probability that the 50% horizon reaches a given threshold before 2200, along with the mean crossing date and 95% credible interval. The 1-month column is the near-term question; the 125-year column is a stress test --- if your model predicts AI can handle tasks that take a human lifetime, you're probably extrapolating too aggressively.

```{python}
#| echo: false
#| label: tbl-horizon-crossing
#| tbl-cap: "Predicted crossing dates for two horizon thresholds (1 month and 125 years), by trajectory model. P(cross) is the posterior probability of crossing before 2200."

from IPython.display import Markdown

labels = {
    "linear": "Linear",
    "quadratic_pos": "Quadratic",
    "xpow": "Power-law",
    "theta_logistic": "Saturating",
}

def _fmt_row(r):
    frac = r["frac_crossing"]
    if frac < 0.01 or pd.isna(r["mean_date"]):
        return f'{frac:.0%}', "---", "---"
    lo = r["ci95_low"] if pd.notna(r["ci95_low"]) else ""
    hi = r["ci95_high"] if pd.notna(r["ci95_high"]) else ""
    cri = f"{lo} -- {hi}" if lo and hi else "---"
    return f'{frac:.0%}', r["mean_date"], cri

df1m = pd.read_csv("blog/_generated/one_month_horizon.csv")
df125 = pd.read_csv("blog/_generated/125y_horizon.csv")

rows = []
for _, r1 in df1m.iterrows():
    trend = r1["theta_trend"]
    label = labels.get(trend, trend)
    p1, d1, c1 = _fmt_row(r1)
    r125 = df125[df125["theta_trend"] == trend]
    if len(r125):
        p2, d2, c2 = _fmt_row(r125.iloc[0])
    else:
        p2, d2, c2 = "---", "---", "---"
    rows.append({
        "Trend": label,
        "1 mo P(cross)": p1,
        "1 mo mean date": d1,
        "1 mo 95% CrI": c1,
        "125 yr P(cross)": p2,
        "125 yr mean date": d2,
        "125 yr 95% CrI": c2,
    })

Markdown(pd.DataFrame(rows).to_markdown(index=False))
```


## The singularity model

I also tried a finite-time singularity specification of the form $\theta \sim \gamma_0 + \gamma_1 x + c / (t^* - x)^\alpha$. But here the data are completely uninformative. The posterior on the singularity date $t^*$ is essentially the prior. 

---

*Code and data: [repository link]*

*References: Kwa et al. (2025), "Measuring AI Ability to Complete Long Tasks,"
[arXiv:2503.14499](https://arxiv.org/abs/2503.14499). Baker (2001), "The basics
of item response theory."*
