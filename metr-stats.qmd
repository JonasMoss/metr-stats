---
title: "Four models that fit the METR data equally well"
author: "Jonas Moss"
date: "2026-02-12"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
    embed-resources: true
execute:
  echo: true
  warning: false
  freeze: auto
jupyter: python3
---

```{python}
# | echo: false
# | output: false
# | label: setup

import json
import math
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from datetime import datetime

# ---------------------------------------------------------------------------
# Paths & specs
# ---------------------------------------------------------------------------
RUNS_ROOT = Path("outputs/runs")

SPECS = {
    "Linear": "time_irt__theta_linear",
    "Quadratic": "time_irt__theta_quadratic_pos",
    "Power-law": "time_irt__theta_xpow",
    "Saturating": "time_irt__theta_theta_logistic",
}

SPEC_COLORS = {
    "Linear": "C0",
    "Quadratic": "C1",
    "Power-law": "C2",
    "Saturating": "C3",
}


def resolve_latest(spec: str) -> Path:
    """Return the concrete run directory for a spec (reads LATEST pointer)."""
    spec_dir = RUNS_ROOT / spec
    run_id = (spec_dir / "LATEST").read_text().strip()
    return spec_dir / run_id


def figdir(spec: str) -> Path:
    return resolve_latest(spec) / "figures"


def diagdir(spec: str) -> Path:
    return resolve_latest(spec) / "diagnostics"


# ---------------------------------------------------------------------------
# Utility functions (from scripts/make_figures.py)
# ---------------------------------------------------------------------------
def logistic(x):
    return 1.0 / (1.0 + np.exp(-np.asarray(x, dtype=float)))


def logit(p):
    return math.log(p / (1.0 - p))


def _format_duration_hours(hours: float) -> str:
    if not np.isfinite(hours) or hours <= 0:
        return ""
    seconds = hours * 3600.0
    if seconds < 60:
        return f"{int(round(seconds))} sec"
    minutes = seconds / 60.0
    if minutes < 60:
        return f"{int(round(minutes))} min"
    if hours < 24:
        v = int(round(hours))
        return f"{v} hour" if v == 1 else f"{v} hours"
    days = hours / 24.0
    if days >= 365:
        years = int(round(days / 365.0))
        return f"{years} year" if years == 1 else f"{years} years"
    if days >= 30:
        months = int(round(days / 30.0))
        return f"{months} month" if months == 1 else f"{months} months"
    v = int(round(days))
    return f"{v} day" if v == 1 else f"{v} days"


def apply_metr_duration_ticks(ax, y_values_hours):
    y = np.asarray(y_values_hours, dtype=float)
    y = y[np.isfinite(y) & (y > 0)]
    if y.size == 0:
        return
    ymin, ymax = float(np.min(y)), float(np.max(y))
    if ymin <= 0 or ymax <= 0:
        return
    ticks = np.array(
        [
            4 / 3600,
            36 / 3600,
            6 / 60,
            1.0,
            10.0,
            4 * 24.0,
            30 * 24.0,
            6 * 30 * 24.0,
            3 * 365 * 24.0,
            18 * 365 * 24.0,
            50 * 365 * 24.0,
            100 * 365 * 24.0,
            500 * 365 * 24.0,
        ]
    )
    lo, hi = ymin / 1.15, ymax * 1.15
    keep = ticks[(ticks >= lo) & (ticks <= hi)]
    if keep.size < 3:
        below = ticks[ticks < lo]
        above = ticks[ticks > hi]
        candidates = []
        if below.size:
            candidates.append(below[-1])
        candidates.extend(keep.tolist())
        if above.size:
            candidates.append(above[0])
        keep = np.array(candidates, dtype=float)
    if keep.size == 0:
        return
    ax.set_yticks(keep)
    ax.set_yticklabels([_format_duration_hours(float(t)) for t in keep])
    ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)


# ---------------------------------------------------------------------------
# Data loaders
# ---------------------------------------------------------------------------
def load_theta_points(spec: str) -> pd.DataFrame:
    df = pd.read_csv(figdir(spec) / "theta_points.csv")
    df["release_date"] = pd.to_datetime(df["release_date"])
    return df


def load_theta_trend(spec: str) -> pd.DataFrame:
    df = pd.read_csv(figdir(spec) / "theta_trend_grid.csv")
    df["date"] = pd.to_datetime(df["date"])
    return df


def load_horizon_grid(spec: str, kind: str = "marginal") -> pd.DataFrame:
    fname = "horizon_grid.csv" if kind == "marginal" else "horizon_grid_typical.csv"
    df = pd.read_csv(figdir(spec) / fname)
    df["date"] = pd.to_datetime(df["date"])
    return df


def load_horizon_points(spec: str, kind: str = "marginal") -> pd.DataFrame:
    fname = "horizon_points.csv" if kind == "marginal" else "horizon_points_typical.csv"
    df = pd.read_csv(figdir(spec) / fname)
    df["release_date"] = pd.to_datetime(df["release_date"])
    return df


def load_meta(spec: str) -> dict:
    return json.loads((resolve_latest(spec) / "fit" / "meta.json").read_text())


# ---------------------------------------------------------------------------
# Plot defaults
# ---------------------------------------------------------------------------
plt.rcParams.update(
    {
        "figure.facecolor": "white",
        "axes.facecolor": "white",
        "font.size": 11,
        "axes.titlesize": 13,
        "axes.labelsize": 12,
    }
)
```

```{python}
# | echo: false
# | output: false
# | label: precompute

from scipy.stats import gaussian_kde

# Doubling-time draws (months)
_dt_draws = pd.read_csv(
    figdir("time_irt__theta_linear") / "doubling_time_draws.csv"
)
dt_months = _dt_draws["doubling_time_years"].values * 12
dt_q025, dt_q50, dt_q975 = np.percentile(dt_months, [2.5, 50, 97.5])

# 125-year crossing dates for TLDR
_df125_pre = pd.read_csv("blog/_generated/125y_horizon.csv")

def _crossing_cri(df, trend):
    r = df[df["theta_trend"] == trend].iloc[0]
    lo = pd.to_datetime(r["ci95_low"])
    hi = pd.to_datetime(r["ci95_high"])
    if lo.day >= 15:
        lo += pd.offsets.MonthBegin(1)
    if hi.day >= 15:
        hi += pd.offsets.MonthBegin(1)
    return lo.strftime("%Y-%m"), hi.strftime("%Y-%m")

cross125_lin_lo, cross125_lin_hi = _crossing_cri(_df125_pre, "linear")
cross125_quad_lo, cross125_quad_hi = _crossing_cri(_df125_pre, "quadratic_pos")
```

## TLDR
I reanalyzed the METR task data using a Bayesian item response theory model. 

* Four trajectory shapes (linear, quadratic, power-law, saturating) fit the existing data equally well but diverge on forecasts. For instance, the 95% credible interval for 125 year-crossing is `{python} cross125_lin_lo` – `{python} cross125_lin_hi` for linear and `{python} cross125_quad_lo` – `{python} cross125_quad_hi` for quadratic.
* Doubling time under the standard linear (exponential growth) model is ~`{python} f"{dt_q50:.1f}"` months, which is similar to METR's estimate. (95% credible interval: `{python} f"{dt_q025:.1f}"`–`{python} f"{dt_q975:.1f}"`).
* Tasks of the same human-time length vary a lot in difficulty. This matters because using "80% success on a typical task of length X" instead of "80% expected success on a random task of length X" makes the predicted crossing dates roughly an order of magnitude smaller. The marginal (random task) version is what's practically relevant but it's less impressive than the typical version METR uses.
* Credible intervals throughout are probably too narrow because I treat human task times as known rather than estimating them as latent variables. I'm doing this because I don't have access to all the raw data, and it could be a big deal.

## METR data

Let's start with a plot that shouldn't be too surprising. Four reasonable models fit the [METR data](https://github.com/METR/eval-analysis-public/tree/main/reports/time-horizon-1-1/data/raw) equally well.[LOO scores differ by at most ~7 points and calibration is nearly identical, with Brier $\approx$ 0.066 across the board.]{.aside} They agree about the past but disagree strongly about the future.


```{python}
# | label: fig-horizon-fan
# | fig-cap: "50%-success horizon vs. release date under four trajectory models."
# | echo: false

fig, ax = plt.subplots(figsize=(9.5, 3.8))

x_max = pd.Timestamp("2029-01-01")

for label, spec_name in SPECS.items():
    g = load_horizon_grid(spec_name).query("p == 0.5").sort_values("date")
    g = g[g["date"] <= x_max]
    ax.plot(
        g["date"], g["t_hours_q50"], color=SPEC_COLORS[label], linewidth=2, label=label
    )
    ax.fill_between(
        g["date"],
        g["t_hours_q025"],
        g["t_hours_q975"],
        color=SPEC_COLORS[label],
        alpha=0.10,
    )

# Model-specific horizon points.
hpts = load_horizon_points(list(SPECS.values())[0]).query("p == 0.5")
ax.errorbar(
    hpts["release_date"],
    hpts["t_hours_q50"],
    yerr=np.vstack(
        [
            hpts["t_hours_q50"] - hpts["t_hours_q025"],
            hpts["t_hours_q975"] - hpts["t_hours_q50"],
        ]
    ),
    fmt="o",
    color="black",
    ecolor="0.35",
    elinewidth=1.5,
    capsize=3,
    alpha=0.7,
    zorder=5,
    markersize=5,
)

# Label a few key models.
LABELS_TO_SHOW = {
    "gpt_4o_inspect": ("GPT-4o", (8, -8)),
    "claude_3_5_sonnet_20241022_inspect": ("Sonnet 3.5 (new)", (8, -8)),
    "claude_opus_4_5_inspect": ("Opus 4.5", (8, 0)),
}
for _, r in hpts.iterrows():
    if r["model"] in LABELS_TO_SHOW:
        txt, ofs = LABELS_TO_SHOW[r["model"]]
        ax.annotate(
            txt,
            (r["release_date"], r["t_hours_q50"]),
            textcoords="offset points",
            xytext=ofs,
            fontsize=7.5,
            color="0.3",
            va="center",
        )

# 1-month reference line.
ax.axhline(
    30 * 24, color="red", linestyle="--", alpha=0.5, linewidth=1.5, label="1 month"
)

ax.set_yscale("log")
ax.set_xlabel("Release date")
ax.set_title("50%-success horizon vs. release date")
ax.set_xlim(pd.Timestamp("2023-01-01"), x_max)
ax.set_ylim(0.03, 365 * 24)  # ~2 min to 1 year
ax.xaxis.set_major_locator(mdates.YearLocator())
ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))

# Duration ticks within the visible range.
tick_hours = np.array(
    [
        6 / 60,
        1.0,
        10.0,
        4 * 24.0,
        30 * 24.0,
        6 * 30 * 24.0,
    ]
)
ax.set_yticks(tick_hours)
ax.set_yticklabels([_format_duration_hours(float(t)) for t in tick_hours])
ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)

ax.legend(fontsize=8, loc="upper left")
fig.tight_layout()
plt.show()
```

These curves are fitted using a Bayesian item response theory model described below. Before describing it let's recall METR's analysis of the time-horizon. They proceed in two stages:

1. **Per-model logistic regression.** For each model $i$, fit $P(\text{success}) = \sigma(\beta_i(\log h_i - \log t_j))$ where $t_j$ is human time for task $j$. Here $h_i$ is the task duration where the curve crosses 50%. When $t_j = h_i$ the argument is zero, so $\sigma(0) = 0.5$. This gives a "horizon score" $h_i$ per model.

2. **An OLS trend.** Regress $\log h_i$ on release date. The slope gives a doubling time of $\approx 4$ months.

This is good modelling and gets the main story right, but there is room for improvement. For instance, the slope $\beta_i$ is per-model rather than per-task (which is unusual), Stage-2 uncertainty doesn't propagate back into Stage 1, making uncertainty estimation tricky, every task of the same length is modelled as equally difficult, and there is only one trajectory shape. 

In this post I make a joint model, adjust some things to be more in line with standard practice, and ask what happens when you try different trajectory shapes. The post is unavoidably somewhat technical, but not so godawful that Claude won't be able to answer any question you have about the methodology. Models are fitted with Stan, 4 chains $\times$ 1000 post-warmup draws, with code available [here]. I intentionally won't go into details about technicalities, e.g. prior choices -- the code contains anything you wish to know and your favourite LLM will figure it out for you. (All priors were chosen by Codex / Claude Code and appear reasonable enough.)

## Psychometrics

The first stage of METR's model is *almost* a 2-parameter logistic model (2PL), the work-horse of educational testing since the 1960s.

So, what kind of problems were the 2PL model designed for? Say you give 200 students a maths exam with 50 questions and record their answers as correct / incorrect. You want to estimate the students' math ability, but raw percent correct scores aren't necessarily very good, as they depend on which questions (easy or hard? relative to which students?) happened to be on the exam.

The 2PL model solves this by giving each student a single ability score ($\theta_i$) and each question two parameters: a *difficulty* ($b_j$, how hard it is) and a *discrimination* ($a_j$, how cleanly it separates strong from weak students). "What is 3*2?" has low discrimination as everyone gets it right regardless of ability. A simple proof-writing question has high discrimination as sufficiently strong students can solve it but poor students have no chance.  

The model estimates all parameters simultaneously via a logistic regression:

$$
P(\text{success} \mid \text{model } i, \text{task } j) = \text{logit}^{-1}\bigl(a_j (\theta_i - b_j)\bigr)
$$

This machinery matters here because METR tasks are like exam questions. They vary in both difficulty and discriminating power, and we need to place all the models on a common ability scale.

## Modeling difficulty

Ability and difficulty parameters $\theta_i, b_j$ in the 2PL are hard to interpret. The scale is arbitrary, and it's not clear what, for instance, a 0.1 increase in ability actually means. Or whether it would be better to take a log-transform of the parameter, etc. A really cool / tubular feature of the METR data is that each task comes with a human time, which is already scaled, transformed, and interpretable. So let's connect human time / task length to difficulty first.

$$
b_j \sim \mathcal{N}(\alpha + \kappa \cdot \log t_j, \;\sigma_b)
$$

Each task's difficulty has a mean that depends on log-length, plus a random component to account for the fact that same-length tasks are not born equal. (METR treats all tasks of identical length as equally hard.) I've estimated $\sigma_b \approx 1.44$, which is quite large, and roughly means that two tasks with the same human-time can differ by several units on the difficulty scale. Compared to the simpler $b_j = \alpha + \kappa \cdot \log t_j$, the random effects widen the credible intervals, so this modeling choice matters for honest uncertainty reporting.

This is a modelling choice that can be wrong. There's no guarantee that difficulty is well-approximated by a linear function of $\log t_j$, so we need diagnostics to check. Here's a residual plot which I find reasonable, with some minor caveats.

```{python}
# | label: fig-difficulty-residuals
# | fig-cap: "Each dot is a task. Residual difficulty (y-axis) measures how much harder or easier a task is for AI than predicted by human-time alone."
# | echo: false

resid = pd.read_csv(diagdir("time_irt__theta_linear") / "difficulty_residuals.csv")

fig, ax = plt.subplots(figsize=(7, 3.5))
ax.scatter(resid["x"], resid["u_hat"], s=18, alpha=0.45, color="C0", edgecolors="none")
ax.axhline(0, color="gray", linestyle="--", linewidth=0.8, alpha=0.6)
ax.set_xlabel(r"Centred log-task length")
ax.set_ylabel(r"Residual difficulty")
fig.tight_layout()
plt.show()
```

Diagnostics aside, this is what $\sigma_b \approx 1.44$ looks like in practice. There's not too much curvature, so I think the log-linear form is decent, though there might be some curvature from -2 on. There is though a weird cluster at the left, which I think can be explained by very easy tasks containing virtually no information about difficulty, but I don't think it's worth inspecting these further / removing points. Overall this looks a-ok to me.

## Modeling model ability

By directly modeling model ability we can try out shapes like exponential, subexponential, superexponential, saturating, and singularity. Forecasts depend a lot on which shape you pick, and the data doesn't really tell you much, so it's not easy to choose between them. Your priors rule here.

Define $x_i = (\text{date}_i - \text{date}_0) / 365.25$, where $\text{date}_0$ is the mean release date (currently Sep 2024), so $x$ is in years. The abilities are modeled as
$$
\theta_i \sim \mathcal{N}\bigl(f(x_i;\, \gamma),\; \sigma_\theta\bigr)
$$

I'm still using a random effect for model ability here, since nobody seriously thinks every model released on the same date must be equally capable. I'm looking at four shapes for $f$:[Quadratic is the simplest choice of superexponential function. One might spin up a story in its favour but using it is somewhat arbitrary. The power-law is the easiest function that can be both super- and subexponential, and the saturating model I've included because why?]{.aside}

| Model | $f(x)$ | Params | Intuition |
|:------|:-------|:------:|:----------|
| Linear | $\gamma_0 + \gamma_1 x$ | 2 | Linear $\theta$ = exponential horizon growth (constant doubling time) |
| Quadratic | $\gamma_0 + \gamma_1 x + \gamma_2 x^2$, $\gamma_2 \geq 0$ | 3 | Superexponential, accelerating growth |
| Power-law | $\gamma_0 + \gamma_1 \tilde{x}^{\alpha}$, $\alpha \in [0.1, 2]$ | 3 | Flexible: sub- or super-exponential |
| Saturating | $\theta_{\min} + \Delta\theta \cdot \text{logit}^{-1}(a + bx)$ | 4 | S-curve ceiling on ability, $\tilde{x}$ is a shifted/scaled version of $x$. |

If METR's GitHub repo contained all the historical data I would also have tried a piecewise linear with a breakpoint around the time of o1, which visually fits the original METR graphs better than a plain linear fit. But since the available data doesn't go that far back I don't need to, and the value of including those early points in a forecasting exercise is questionable anyway. Getting hold of the latest data points is more important.

All models share the same 2PL likelihood and task parameters ($b_j$, $a_j$,
$\alpha$, $\kappa$, $\sigma_b$). Only the $\theta$-prior changes.

### Implied doubling time

I get `{python} f"{dt_q50:.1f}"` months (95% CrI: `{python} f"{dt_q025:.1f}"`--`{python} f"{dt_q975:.1f}"`), which is similar to METR's v1.1 estimate of ~4.3 months. Of course, doubling time only makes sense for the linear model above, as the doubling time of the other models varies with time.

```{python}
#| label: fig-doubling-time
#| fig-cap: "Posterior distribution of the horizon doubling time under the linear model."
#| echo: false

from scipy.stats import gaussian_kde

draws = pd.read_csv(
    figdir("time_irt__theta_linear") / "doubling_time_draws.csv"
)
dt_months = draws["doubling_time_years"].values * 12

kde = gaussian_kde(dt_months, bw_method="silverman")
x = np.linspace(dt_months.min() - 0.3, dt_months.max() + 0.3, 400)

q025, q50, q975 = np.percentile(dt_months, [2.5, 50, 97.5])

fig, ax = plt.subplots(figsize=(7, 3.2))
ax.fill_between(x, kde(x), alpha=0.15, color="C0")
ax.plot(x, kde(x), color="C0", linewidth=2)
ax.axvline(q50, color="black", linewidth=1.5, label=f"Median = {q50:.1f} months")
ax.axvline(q025, color="black", linewidth=1, linestyle="--", alpha=0.6,
           label=f"95% CrI: [{q025:.1f}, {q975:.1f}]")
ax.axvline(q975, color="black", linewidth=1, linestyle="--", alpha=0.6)
ax.set_xlabel("Doubling time (months)")
ax.set_ylabel("Posterior density")
ax.legend(fontsize=9)
fig.tight_layout()
plt.show()
```


### Crossing thresholds

Each model except the saturating model will cross any threshold if the date is large enough. Here's posteriors for the 50% crossing across our models. The saturating models usually never crosses the 1 month and 125 year thresholds since it saturates too fast. 

```{python}
# | echo: false
# | label: tbl-horizon-crossing
# | tbl-cap: "Predicted crossing dates for two horizon thresholds (1 month and 125 years), by trajectory model."

from IPython.display import Markdown


def _to_ym(date_str):
    """Round a YYYY-MM-DD string to nearest month, return YYYY-MM."""
    if not date_str or pd.isna(date_str) or date_str == "---":
        return "---"
    d = pd.to_datetime(date_str)
    if d.day >= 15:
        d = d + pd.offsets.MonthBegin(1)
    return d.strftime("%Y-%m")


labels = {
    "linear": "Linear",
    "quadratic_pos": "Quadratic",
    "xpow": "Power-law",
}


def _fmt_row(r):
    frac = r["frac_crossing"]
    if frac < 0.01 or pd.isna(r["mean_date"]):
        return "---", "---"
    lo = _to_ym(r["ci95_low"]) if pd.notna(r["ci95_low"]) else ""
    hi = _to_ym(r["ci95_high"]) if pd.notna(r["ci95_high"]) else ""
    cri = f"{lo} -- {hi}" if lo and hi else "---"
    return _to_ym(r["mean_date"]), cri


df1m = pd.read_csv("blog/_generated/one_month_horizon.csv")
df125 = pd.read_csv("blog/_generated/125y_horizon.csv")

rows = []
for _, r1 in df1m.iterrows():
    trend = r1["theta_trend"]
    if trend not in labels:
        continue
    label = labels[trend]
    d1, c1 = _fmt_row(r1)
    r125 = df125[df125["theta_trend"] == trend]
    if len(r125):
        d2, c2 = _fmt_row(r125.iloc[0])
    else:
        d2, c2 = "---", "---"
    rows.append((label, d1, c1, d2, c2))

html50 = '<table style="width:100%">\n<thead>\n'
html50 += '<tr><th rowspan="2">Trend</th>'
html50 += '<th colspan="2" style="text-align:center;border-bottom:none">1 month</th>'
html50 += '<th colspan="2" style="text-align:center;border-bottom:none">125 years</th></tr>\n'
html50 += '<tr><th>Mean</th><th>95% CrI</th><th>Mean</th><th>95% CrI</th></tr>\n'
html50 += '</thead>\n<tbody>\n'
for label, d1, c1, d2, c2 in rows:
    html50 += f'<tr><td>{label}</td><td>{d1}</td><td>{c1}</td><td>{d2}</td><td>{c2}</td></tr>\n'
html50 += '</tbody>\n</table>'
Markdown(html50)
```


### What about 80% success? 

Everything above uses 50% success, but METR also cares about 80% success and fits a separate model for that. We don't need to do that here since the model estimation doesn't really depend on success rates at all. We'll just calculate the 80% success using posterior draws instead.

But there's actually two reasonable ways to define "80% success," and they give different answers.

1. **Typical:** Pick a task of average difficulty for its length. Can the model solve it 80% of the time? This is roughly what METR computes.

2. **Marginal:** Pick a random task of that length. What's the expected success rate? Because some tasks are much harder than average, the hard ones drag down the average more than easy ones push it up.

At 50% the two definitions agree. But at 80%, the gap is roughly an order of magnitude! 

```{python}
# | label: fig-marginal-typical
# | fig-cap: "Horizon forecasts at 80% success probability. Left: linear trajectory. Right: quadratic (constrained)."
# | echo: false

panel_specs = [
    ("Linear", "time_irt__theta_linear"),
    ("Quadratic", "time_irt__theta_quadratic_pos"),
]
x_max = pd.Timestamp("2029-01-01")

fig, axes = plt.subplots(1, 2, figsize=(9.5, 4.0), sharey=True)

for ax, (panel_label, spec_name) in zip(axes, panel_specs):
    marg = load_horizon_grid(spec_name, kind="marginal")
    typ = load_horizon_grid(spec_name, kind="typical")

    # 50% (marginal ≈ typical) — thin reference.
    g50 = marg.query("p == 0.5").sort_values("date")
    g50 = g50[g50["date"] <= x_max]
    ax.plot(
        g50["date"], g50["t_hours_q50"], color="0.6", linewidth=1.2, label="50% success"
    )
    ax.fill_between(
        g50["date"], g50["t_hours_q025"], g50["t_hours_q975"], color="0.6", alpha=0.06
    )

    # 80% typical.
    g80t = typ.query("p == 0.8").sort_values("date")
    g80t = g80t[g80t["date"] <= x_max]
    ax.plot(
        g80t["date"], g80t["t_hours_q50"], color="C0", linewidth=2, label="80% typical"
    )
    ax.fill_between(
        g80t["date"], g80t["t_hours_q025"], g80t["t_hours_q975"], color="C0", alpha=0.10
    )

    # 80% marginal.
    g80m = marg.query("p == 0.8").sort_values("date")
    g80m = g80m[g80m["date"] <= x_max]
    ax.plot(
        g80m["date"],
        g80m["t_hours_q50"],
        color="C3",
        linewidth=2,
        linestyle="--",
        label="80% marginal",
    )
    ax.fill_between(
        g80m["date"], g80m["t_hours_q025"], g80m["t_hours_q975"], color="C3", alpha=0.10
    )

    # Model-specific horizon points at 80%.
    pts_typ = load_horizon_points(spec_name, kind="typical").query("p == 0.8")
    pts_marg = load_horizon_points(spec_name, kind="marginal").query("p == 0.8")
    ax.errorbar(
        pts_typ["release_date"],
        pts_typ["t_hours_q50"],
        yerr=np.vstack(
            [
                pts_typ["t_hours_q50"] - pts_typ["t_hours_q025"],
                pts_typ["t_hours_q975"] - pts_typ["t_hours_q50"],
            ]
        ),
        fmt="o",
        color="C0",
        ecolor="C0",
        elinewidth=1.2,
        capsize=2,
        alpha=0.6,
        zorder=5,
        markersize=4,
    )
    ax.errorbar(
        pts_marg["release_date"],
        pts_marg["t_hours_q50"],
        yerr=np.vstack(
            [
                pts_marg["t_hours_q50"] - pts_marg["t_hours_q025"],
                pts_marg["t_hours_q975"] - pts_marg["t_hours_q50"],
            ]
        ),
        fmt="s",
        color="C3",
        ecolor="C3",
        elinewidth=1.2,
        capsize=2,
        alpha=0.6,
        zorder=5,
        markersize=4,
    )

    ax.axhline(
        30 * 24, color="red", linestyle="--", alpha=0.3, linewidth=1, label="1 month"
    )

    ax.set_yscale("log")
    ax.set_title(panel_label, fontsize=12)
    ax.set_xlim(pd.Timestamp("2023-01-01"), x_max)
    ax.set_ylim(1e-3, 365 * 24)
    ax.xaxis.set_major_locator(mdates.YearLocator())
    ax.xaxis.set_major_formatter(mdates.DateFormatter("%Y"))

    tick_hours = np.array(
        [1 / 60, 6 / 60, 1.0, 10.0, 4 * 24.0, 30 * 24.0, 6 * 30 * 24.0]
    )
    ax.set_yticks(tick_hours)
    ax.set_yticklabels([_format_duration_hours(float(t)) for t in tick_hours])
    ax.grid(True, which="major", axis="y", linestyle="--", linewidth=0.8, alpha=0.25)

axes[0].set_ylabel("")
axes[0].legend(fontsize=7.5, loc="upper left")
axes[1].set_xlabel("Release date")
axes[0].set_xlabel("Release date")
fig.tight_layout()
plt.show()
```

So, on one hand, it's the variance ($\sigma_b \approx 1.44$) alone that causes these two plots to be necessary in the setting of our model. But on the other hand, the difference is not really a consequence of modelling. Some tasks of the same human-time length vary a lot in how hard they are for our models, and a phenomenon like this would happen for *any* model that's actually honest about this.

I'd say the "correct" horizon to talk about is the marginal one. "Typical" is optimistic since it works on tasks of average difficulty for its length only. On the other hand, marginal accounts for the full spread of real tasks, so it is what's practically relevant for predicting the probability of success on an arbitrary task of some length. That said, frontier performance of $6$ minutes does sound sort of short? I am not really sure. I ask Claude to do short coding and stats tasks frequently, and I do get subtly wrong responses. I usually attribute these to imprecise prompting but I could be wrong.

Anyway, the predicted crossing dates at 80% success:

```{python}
# | echo: false
# | label: tbl-horizon-80-1mo
# | tbl-cap: "Predicted date when the 80%-success horizon reaches 1 month. The saturating model is omitted (almost never crosses)."

df_typ_1m = pd.read_csv("blog/_generated/one_month_horizon_p80_typical.csv")
df_marg_1m = pd.read_csv("blog/_generated/one_month_horizon_p80_marginal.csv")

_labels80 = {
    "linear": "Linear",
    "quadratic_pos": "Quadratic",
    "xpow": "Power-law",
}

def _get80(df, trend):
    sub = df[df["theta_trend"] == trend]
    if not len(sub):
        return "---", "---"
    r = sub.iloc[0]
    if r["frac_crossing"] < 0.01 or pd.isna(r["mean_date"]):
        return "---", "---"
    lo = _to_ym(r["ci95_low"]) if pd.notna(r["ci95_low"]) else ""
    hi = _to_ym(r["ci95_high"]) if pd.notna(r["ci95_high"]) else ""
    cri = f"{lo} -- {hi}" if lo and hi else "---"
    return _to_ym(r["mean_date"]), cri

rows_1m = []
for trend, label in _labels80.items():
    td, tc = _get80(df_typ_1m, trend)
    md, mc = _get80(df_marg_1m, trend)
    rows_1m.append((label, td, tc, md, mc))

h = '<table style="width:100%">\n<thead>\n'
h += '<tr><th rowspan="2">Trend</th>'
h += '<th colspan="2" style="text-align:center;border-bottom:none">Typical</th>'
h += '<th colspan="2" style="text-align:center;border-bottom:none">Marginal</th></tr>\n'
h += '<tr><th>Mean</th><th>95% CrI</th><th>Mean</th><th>95% CrI</th></tr>\n'
h += '</thead>\n<tbody>\n'
for label, td, tc, md, mc in rows_1m:
    h += f'<tr><td>{label}</td><td>{td}</td><td>{tc}</td><td>{md}</td><td>{mc}</td></tr>\n'
h += '</tbody>\n</table>'
Markdown(h)
```

```{python}
# | echo: false
# | label: tbl-horizon-80-125y
# | tbl-cap: "Predicted date when the 80%-success horizon reaches 125 years. The saturating model is omitted (never crosses)."

df_typ_125 = pd.read_csv("blog/_generated/125y_horizon_p80_typical.csv")
df_marg_125 = pd.read_csv("blog/_generated/125y_horizon_p80_marginal.csv")

rows_125 = []
for trend, label in _labels80.items():
    td, tc = _get80(df_typ_125, trend)
    md, mc = _get80(df_marg_125, trend)
    rows_125.append((label, td, tc, md, mc))

h2 = '<table style="width:100%">\n<thead>\n'
h2 += '<tr><th rowspan="2">Trend</th>'
h2 += '<th colspan="2" style="text-align:center;border-bottom:none">Typical</th>'
h2 += (
    '<th colspan="2" style="text-align:center;border-bottom:none">Marginal</th></tr>\n'
)
h2 += "<tr><th>Mean</th><th>95% CrI</th><th>Mean</th><th>95% CrI</th></tr>\n"
h2 += "</thead>\n<tbody>\n"
for label, td, tc, md, mc in rows_125:
    h2 += f"<tr><td>{label}</td><td>{td}</td><td>{tc}</td><td>{md}</td><td>{mc}</td></tr>\n"
h2 += "</tbody>\n</table>"
Markdown(h2)
```

Make of this what you want, but let's go through one scenario. Let's say I'm a believer in superexponential models with no preference between quadratic and power-law, so I have 50-50 weighting on those. Suppose also I believe that 125 years is the magic number for the auto-coder of [AI Futures](https://www.aifuturesmodel.com/), but I prefer $80\%$ to $50\%$ as the latter is too brittle. Then, using the arguably correct marginal formulation, my timeline has mean roughly January 2031, but the typical framework yields mean ~2030 instead. And this isn't too bad, just a difference of one year! The linear model is similar -- timelines are pushed out roughly 1.5 year.  

## Modeling $t_j$

The main weakness of this analysis is treating task times $t_j$ as known. METR takes the geometric mean of (usually 2-3) successful human baselines per task and treats it as a known quantity, discarding uncertainty. But we can model $t_j$ as a latent variable informed by the reported baselines. This would be straightforward to implement in Stan, and would give a more honest picture of what the data actually support, as all credible intervals will be widened. 

It's probable that we'd observe smaller differences between the typical and marginal plots at $80\%$ horizon by modelling the $t_j$ s properly, as more of the variance would likely be absorbed by the actual real uncertainty in $t_j$. I'm not sure how big the effect would be, but getting hold of the data or doing a short simulation would help.  

A technical point: When modelling $t_j$ I would also try a Weibull distribution instead of log-normal, since the log-normal is typically more heavy-tailed and the Weibull has stronger intuitive justification as a general distribution of task completion times.

## Notes and remarks

* I also tried a finite-time singularity model of the form $\theta \sim \gamma_0 + \gamma_1 x + c / (t^* - x)^\alpha$. The posterior on the singularity date $t^*$ didn't really move from the prior at all. This is no surprise, it just means the data are uninformative.
* There are loads of other knobs one could turn. Perhaps you could introduce a discrimination parameter that varies by model and task, together with a hierarchical prior. Perhaps you could make discrimination a function of time, etc. I doubt any of these would change the picture much, if at all. The model fit is good enough as it is, even if the uncertainty is likely too small. That said, I don't want to dissuade anyone from trying!
* The power-law model does in principle support both sub- and superexponential trajectories ($\alpha < 1$ and $\alpha > 1$ respectively, where $\alpha = 1$ is linear model). The posterior puts $P(\alpha < 1) \approx 4\%$, so the data does not support subexponential growth. At least when using this model.
* There's plenty of best-practice stuff I haven't done, such inspecting the importance of priors. (But we have a lot of data, and I wouldn't expect them to matter too much.) 
* (Claude point): Having Claude code or Codex write up graphs and code is amazing, but it is ... irritating to back and clean up their "accidental" rewriting of my text. That said, any LLM-ese in this post is my responsibility.